import requests
import os
import time
import re
import random
from urllib.parse import quote
from fake_useragent import UserAgent


class BaiduImageDownloader:
    def __init__(self, keyword, save_dir='downloads', max_pages=3, delay=1.5, proxy=None):
        self.keyword = keyword
        self.save_dir = save_dir
        self.max_pages = max_pages
        self.delay = delay
        self.proxy = proxy
        self.ua = UserAgent()

        # åˆå§‹åŒ–è®¾ç½®
        self._prepare()

    def _prepare(self):
        """åˆ›å»ºç›®å½•å¹¶æ£€æŸ¥ç¯å¢ƒ"""
        if not os.path.exists(self.save_dir):
            os.makedirs(self.save_dir)
        if not hasattr(self, 'session'):
            self.session = requests.Session()
            self.session.verify = False  # å…³é—­SSLéªŒè¯ï¼ˆå¿…è¦æ—¶ï¼‰

    def _get_headers(self):
        """ç”ŸæˆåŠ¨æ€è¯·æ±‚å¤´"""
        return {
            'User-Agent': self.ua.chrome,
            'Referer': 'https://image.baidu.com/',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Connection': 'keep-alive',
            'DNT': '1'  # å¼€å¯ç¦æ­¢è¿½è¸ª
        }

    def _fetch_page(self, page):
        """è·å–å•é¡µæ•°æ®"""
        encoded_word = quote(self.keyword)
        base_url = 'https://image.baidu.com/search/index?tn=baiduimage&ct=201326592&lm=-1&cl=2&ie=utf8&word=%E5%B0%8F%E5%A7%90%E5%A7%90%E5%9B%BE%E7%89%87&fr=ala&ala=1&alatpl=normal&pos=0&dyTabStr=MCwxMiwzLDEsMiwxMyw3LDYsNSw5'

        params = {
            'tn': 'resultjson_com',
            'ipn': 'rj',
            'fp': 'result',
            'queryWord': encoded_word,
            'word': encoded_word,
            'pn': page * 30,
            'rn': 30,
            'gsm': hex(page * 30)[2:],
            't': int(time.time() * 1000)  # åŠ¨æ€æ—¶é—´æˆ³
        }

        try:
            # ä½¿ç”¨éšæœºå»¶è¿Ÿï¼ˆ0.5å€åˆ°1.5å€åŸºç¡€å»¶è¿Ÿï¼‰
            time.sleep(self.delay * random.uniform(0.5, 1.5))

            response = self.session.get(
                base_url,
                headers=self._get_headers(),
                params=params,
                proxies={'http': self.proxy, 'https': self.proxy} if self.proxy else None,
                timeout=(10, 15)
            )

            # å“åº”éªŒè¯
            if response.status_code != 200:
                print(f"[é”™è¯¯] é200çŠ¶æ€ç ï¼š{response.status_code}")
                return None

            content = response.text.strip()
            if not content or '<html>' in content.lower():
                print("[è­¦å‘Š] æ”¶åˆ°å¯èƒ½çš„é‡å®šå‘é¡µé¢")
                return None

            return response.json()

        except Exception as e:
            print(f"[ç½‘ç»œé”™è¯¯] ç¬¬{page + 1}é¡µï¼š{str(e)}")
            if hasattr(response, 'text'):
                print(f"å“åº”ç‰‡æ®µï¼š{response.text[:200]}")
            return None

    def _extract_urls(self, json_data):
        """è§£æå›¾ç‰‡çœŸå®åœ°å€"""
        urls = []
        for item in json_data.get('data', []):
            if not isinstance(item, dict):
                continue

            # å¤šä¸ªå¯èƒ½åŒ…å«çœŸå®URLçš„å­—æ®µ
            for field in ['hoverURL', 'thumbURL', 'middleURL', 'pic_url', 'objURL']:
                url = item.get(field)
                if url and re.match(r'^https?://', url):
                    urls.append(url)
                    break
        return urls

    def _download_image(self, url, save_path):
        """ä¸‹è½½å•å¼ å›¾ç‰‡"""
        try:
            res = self.session.get(
                url,
                headers={'Referer': 'https://image.baidu.com/'},
                stream=True,
                timeout=10,
                allow_redirects=True
            )

            if res.status_code == 200:
                # æ™ºèƒ½è·å–æ–‡ä»¶ç±»å‹
                content_type = res.headers.get('Content-Type', '')
                ext_match = re.search(r'image/(\w+)', content_type)
                ext = ext_match.group(1) if ext_match else 'jpg'

                with open(f"{save_path}.{ext}", 'wb') as f:
                    for chunk in res.iter_content(2048):
                        f.write(chunk)
                return True
        except Exception as e:
            print(f"[ä¸‹è½½å¤±è´¥] {url[:50]}... - {str(e)}")
            return False

    def run(self):
        """æ‰§è¡Œä¸»ç¨‹åº"""
        print(f"ğŸ” å¼€å§‹æŠ“å–å…³é”®è¯ï¼š{self.keyword}")

        for page in range(self.max_pages):
            print(f"\nğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {page + 1}/{self.max_pages} é¡µ")

            json_data = self._fetch_page(page)
            if not json_data:
                print("â­ï¸ è·³è¿‡æœ¬é¡µ")
                continue

            image_urls = self._extract_urls(json_data)
            print(f"ğŸ–¼ï¸ å‘ç° {len(image_urls)} å¼ å›¾ç‰‡")

            for idx, url in enumerate(image_urls):
                filename = f"{self.keyword}_p{page + 1}_{idx + 1}"
                save_path = os.path.join(self.save_dir, filename)

                print(f"â¬‡ï¸ æ­£åœ¨ä¸‹è½½ï¼š{filename}  åŸåœ°å€ï¼š{url[:40]}...")
                if self._download_image(url, save_path):
                    print("âœ… ä¸‹è½½æˆåŠŸ")
                else:
                    print("âŒ ä¸‹è½½å¤±è´¥")

                time.sleep(random.uniform(0.8, 1.2))  # ä¸‹è½½é—´éš”

        print("\nğŸ‰ ä»»åŠ¡å®Œæˆï¼ä¿å­˜ç›®å½•ï¼š", os.path.abspath(self.save_dir))


if __name__ == '__main__':
    # ä½¿ç”¨æ–¹æ³•ï¼š
    downloader = BaiduImageDownloader(
        keyword="è‡ªç„¶é£å…‰",  # æœç´¢å…³é”®è¯
        save_dir="nature",  # ä¿å­˜ç›®å½•
        max_pages=2,  # æŠ“å–é¡µæ•°ï¼ˆæ¯é¡µçº¦30å¼ ï¼‰
        delay=2.5,  # æ¨èè®¾ç½®2-5ç§’å»¶è¿Ÿ
        # proxy='http://user:pass@host:port'  # å¦‚æœéœ€è¦ä»£ç†
    )
    downloader.run()
